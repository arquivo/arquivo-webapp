<?xml version="1.0" encoding="ISO-8859-1"?>
<document>
  <properties>
    <title>Best Practises</title>
    <author email="stack at archive dot org">stack</author>
    <revision>$Id: practices.xml 1342 2006-12-07 22:27:03Z uid143487 $</revision>
  </properties>
  <body>


<section name="Praxis"> 

<p>
This page is a list in no particular of things learned over time indexing here at
the Archive (Its a little spare for now but we'll add over time). 
</p>

<section name="Undo all external dependencies">
<p>At the Archive, we let go of all NFS mounts on slaves and we even hardcoded DNS so the
cluster had minimal dependency on external services.  On too many occasions, a temporary NFS stall
or a DNS outage killed jobs that had been running 24 hours or more.
</p>
</section>

<section name="Make all slave nodes the same in a cluster">
<p>Save yourself headache by ensuring all nodes have the same hardware profile
-- RAM, number and size of disks -- and that they have the exact same versions
of operating system and the same software (and versions) installed all around.
Hadoop sort of expects the cluster to be homogeneous so giving it something
otherwise will only make your indexing life the harder (And cluster computing
isn't easy at the best of times).
</p>
</section>

<section name="Choose how many ARCs per segment carefully">
<p>An ARC per segment is probably too small.  Tens of thousands of ARCs per
segment, while it makes for less objects to manage, the product will probably
be too large to download to local disk from HDFS or will make for indices
that can't be merged within the confines of a local disk, etc.  Experiment
to find the size best suited to your setup.
</p>
</section>

</section>
</body>
</document>

