<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//OASIS//DTD DocBook XML V4.2//EN"
"http://www.oasis-open.org/docbook/xml/4.2/docbookx.dtd">
<article>
  <title>ARC Collection Search Requirements</title>

  <articleinfo>
    <date>$Date: 2005-01-27 22:45:27 +0000 (Thu, 27 Jan 2005) $</date>

    <authorgroup>
      <editor>
        <affiliation>
          <orgname>Internet Archive</orgname>
        </affiliation>

        <firstname>Michael</firstname>

        <surname>Stack</surname>

        <email>stack at archive dot org</email>
      </editor>
    </authorgroup>

    <revhistory>
      <revision>
        <revnumber>$Revision: 79 $</revnumber>
        <date>$Date: 2005-01-27 22:45:27 +0000 (Thu, 27 Jan 2005) $</date>
        <revremark>
        $Log$
        Revision 1.18  2005/01/27 22:45:27  stack-sf
        * src/docs/arc_collection_search.xml
            Edit.

        Revision 1.17  2005/01/27 22:29:41  stack-sf
        * src/docs/arc_collection_search.xml
            Update section on bridges.  Add examples.

        </revremark>
      </revision>
      <revision>
        <revnumber>1.16</revnumber>
        <date>2005/01/27 21:53:20</date>
        <revremark>
        Incorporated second round of feedback on Sverre Bang (In particular
        filled out section on 'bridges').
        </revremark>
      </revision>
      <revision>
        <revnumber>1.5</revnumber>
        <date>2004/12/16</date>
        <revremark>
            Incorporate feedback from Sverre Bang.
            Go against the IIPC 'An Architectural Framework for the
            IIPC Toolset' document.
        </revremark>
      </revision>
      <revision>
        <revnumber>1.4</revnumber>
        <date>2004/12/12</date>
        <revremark>
            Ready for first circulation.
        </revremark>
      </revision>
    </revhistory>
  </articleinfo>

  <abstract>
    <para>Requirements for a nutch-based search tool to go 
    against repositories of 100 million documents.
    </para>
  </abstract>

  <sect1 id="introduction">
  <para>These are requirements for a delivery that can search collections of at
  least 100 million documents. 100 million is approximately the size of a
  repository that can live on a single machine. 
  </para>
    
    <para>The described delivery will be used searching collections of
  <ulink url="http://archive.org">Internet Archive</ulink> (IA)
  <ulink 
  url="http://www.archive.org/web/researcher/ArcFileFormat.php">ARC</ulink>
  files -- each ARC file being ~100megabytes of compressed, concatenated,
  harvested web content -- and as the search engine plugin in an NWA Toolset
  <citation><xref linkend="nwadf" /></citation>.
  </para>  

  <para>These requirements are informed by the
  <ulink url="#toolset" >Architectural Framework for the IIPC Toolset</ulink>
  document.  Key
  features of the described IIPC Toolset search are included below. The
  long-term intent is that the delivery -- with additional work that puts the
  delivery behind a yet-to-be-described abstracted API -- can evolve to serve
  as a search engine component in an IIPC Toolset.
  </para>

    <title>Introduction</title>
    <sect2 id="scope">
      <title>Scope</title>
      <sect3><title>What to Index</title>
      <sect4><title>ARC Collections</title>
        <para>Harvested web content as txt, html, xml, pdf, msword, etc., 
        written in multiple languages (and encodings),
        stored in sets of ARC files.  Files will be on the local filesystem
        or fetchable via HTTP.</para>
     </sect4>
     <sect4>
        <title>NWA Document Format</title>
        <para>Files written in NWA Document Format
        <citation><xref linkend="nwadf" /></citation>.
        Files will be on the local filesystem or fetchable via HTTP.</para>
        </sect4>
     <sect4>
        <title>Network-accessible documents using a defined API</title>
        <para>The IIPC is developing a repository API which will
        allow listing and fetching of repository content.
        </para>
        </sect4>
      </sect3>

      <sect3 id="timeline">
        <title>Timeline</title>
        <sect4 id="firststage"><title>First Stage</title>
            <para>First stage will be getting nutch to reliably index an
            ARC collection of 100 million documents in January, 2005.
            </para>
        </sect4>
        <sect4 id="secondstage"><title>Second Stage</title>
            <para>Second stage will be adding of below features not present
            in native nutch in the months following January, 2005.
            </para>
        </sect4>
      </sect3>

      <sect3 id="stakeholder">
        <title>Stakeholders</title>
  <para>These requirements have been drafted by the
  <ulink url="http://archive.org">Internet Archive</ulink> for the IA
  and for <ulink url="http://netpreserve.org">IIPC</ulink> Framework Working
  Group.
  </para>
      </sect3>

      <sect3 id="Constraint">
        <title>Constraints</title>
        <sect4 id="nutch"><title>Nutch</title>
        <para>Previous survey work and prototyping
        <citation><xref linkend="crawlprojects" /></citation>
        favors a delivery based on 
        <ulink url="http://nutch.org">nutch</ulink>, an open-source
        search solution.
        </para>
        </sect4>
      </sect3>
    </sect2>


  </sect1>
  <sect1 id="reqs">
    <title>Requirements</title>
    <sect2 id="general"><title>General Requirements</title>
    <sect3 id="doc_src"><title>Accessing Document Sources</title>
    <subtitle>Bridges to Document Collections</subtitle>
        <para>There are two channels that need to be bridged
        connecting a search engine and a document repository: there is the 
        indexing channel and the query-time search result resolution
        channel. Regards the latter, it is
        assumed that repository content is always accessible via HTTP; that
        the search engine search results can just return an URL that points
        into the repository and that clicking on the URL pulls the content
        from the repository (If the content sits behind an API, an HTTP
        proxy will need to be inserted).  Given the previous assumption,
        there is no need to discuss further search-time bridging. 
        The rest of this section discusses the indexing channel.
        </para>

        <para>One step in the multi-step Nutch indexing process is the
        writing of the content to index into Nutch 'segments'.
        The usual way in which this is done in Nutch is that 
        the built-in Nutch crawler runs and fetches documents 
        writing them to disk into Nutch 'segments'.
        Subsequent Nutch processing steps operate on the downloaded
        'segments' to create search indexes and text snippets to display
        in search results.
        </para>

        <para>Running the Nutch fetcher to access the targeted repository
        types -- e.g. Internet Archive ARCs, Directories of NWA
        Document Format instances, or documents in a repository
        accessed via an IIPC API -- may not be an option.  The
        repository content may not be fetchable via HTTP or fetching via 
        HTTP will be less than optimal. Alternate means of manufacturing the
        nutch 'segments' out of the targeted repository content will need to
        be developed.  Such 'bridging' tools  will know how to pull
        efficently from the targeted repository and make Nutch 'segments'
        of the fetched content.  An indexing 'bridging' tool per repository
        content type will need to be developed.  Such repository indexing
        bridging tools encapsulate all repository-accessing details so
        nowhere else in Nutch need know particulars on the repository being
        indexed.</para>

        <para>For example, an ARC Collection 'bridge' would know how to 
        find ARCs on disk and rewrite their content into Nutch 'segments'.
        Another bridge example would know how to pull repository content via an
        IIPC API.
        </para>

        <sect4 id="arc_collection_reader">
        <title>ARC Collection Iterator</title>
            <para>Required is an indexing 'bridge' to go against a
            collection of ARCs and feed Nutch extracted content.
            Must be a means of running the tool in 
            <emphasis>index create</emphasis> and 
            <emphasis>append mode</emphasis> 
            (the latter adds new documents/collections to an extant index).
            </para>
        </sect4>
        <sect4 id="nwa_docs">
            <title>NWA Document Format Repository</title>
            <para>Required is demonstration of how 
            an indexing 'bridge' to go against a
            collection of NWA Document Format instances could be built.
            This bridge is not required as part of this deliverable.
            What is required is demonstration that such a bridge could
            be made work in the delivery.
            Must be a means of running the tool in 
            <emphasis>index create</emphasis> and 
            <emphasis>append mode</emphasis> 
            (the latter adds new documents/collections to an extant index).
            </para>
        </sect4>
        <sect4 id="other_bridges">
            <title>Other bridges</title>
            <para>Required is demonstration of how 
            indexing 'bridges' to go against an (as-yet-undefined)
            IIPC API or to to fetch using the NWA Retriever -- a webapp
            that lists the contents of a repository that allows HTTP GETs
            with a document ID to pull particular documents -- would be built.
            Such bridges are not required as part of the deliverable.
            What is required is demonstration that such bridges could
            be made work in the delivery.
            Must be a means of running the bridges in 
            <emphasis>index create</emphasis> and 
            <emphasis>append mode</emphasis> 
            (the latter adds new documents/collections to an extant index).
            </para>

        </sect4>
        <sect4 id="onerepo"><title>One Repository Type Only</title>
        <para>For the foreseeable future, a particular deployment will be set
        against one repository-type only:
        E.g. A delivery will be configured to go against an ARC collection or 
        directories of NWA Documents.  It will not be expected to
        do both in the one installation.</para>
            </sect4>
    </sect3>
        <sect3 id="i18n">
            <title>Internationalization</title>
                    <para>The delivery cannot support english-only
                    or single-byte-only indexing and searching.  
                    The immediate, intended audience natively speaks at least
                    the following languages: norwegian, swedish, french,
                    english, icelandic, and danish.  The content to index
                    will be written predominately in these languages
                    but the search engine will also be expected to do a
                    reasonable job indexing content harvested off the wide
                    open web.
                    </para>


                <sect4 id="search"><title>Indexing and Querying
                        Language/Encodings</title>
                    <para>Content will come in multiple
                    encodings/languages.  The delivery must be able to
                    exploit encoding
                    volunteered in HTML HEAD META tags and the encoding
                    attribute in the first line of xml documents.  It then
                    must be able to respect the volunteered encoding
                    and digest documents of varied encodings:
                    ISO-8859-1 through 15, UTF-*, and their windows varients
                    of the previous.</para>

                    <para>The search engine must be able to
                    tokenize at least the european languages whether content
                    is rendored in single-byte or multibyte encodings
                    (Successful tokenization of Chinese, Japanese, Korean
                    and Vietnamese -- the asian languages -- is not required).
                    </para>
                </sect4>
                    <sect4 id="localizable">
                    <title>Localizable</title>
                    <para>It is required that the delivery 
                    be localizable without necessitating
                    rewriting core aspects of the search engine or interface.
                    To facilitate localization, the data strings that
                    make up the language content of the delivery -- if any --
                    should be
                    isolated from code stored in distinct resource/data files.
                    </para>
                    <para>There is no requirement that the search engine come
                    in any language other than english (We can do any
                    needed localizations ourselves).
                    </para>
                    </sect4>
        </sect3>   
      <sect3 id="oss">
        <title>Open-Source</title>
            <para>The delivery must be open source software.
            </para>
      </sect3>


    </sect2>

    <sect2 id="queries">
        <title>Queries and Expected Search Result Behavior</title>
        <para>Below are the types of queries we must be able to run.</para>
        <sect3 id="urlonly"><title>Query string</title>
            <para>A string of search terms to find.  
            Exact match is sufficent (No requirement for stemming, etc.).
            The query will be written in a european language in UTF-8.</para>

            <para>Expected result is a result list of documents each of which
            contain the queried terms.
            </para>
        </sect3>
        <sect3 id="urlplusdate"><title>Query string plus a specific date</title>
            <para>A means of specifying a query and a specific date.
            Expected search result will be documents that contain
            queried terms AND that are of the specified date.
            </para>
        </sect3>
        <sect3 id="daterange"><title>Query string plus a date range</title>
            <para>It is required that there be a means of narrowing
            the results returned such that all search result documents
            contain the queried terms and 
            are within a specified date range.
            The date range should have an hour granularity.
            Finer than this would be a nice-to-have but is not required.
            </para>
        </sect3> 
        <sect3 id="daterange_before_after"><title>Query string before or after
                a date</title>
            <para>Hits are  documents that contain the queried terms and 
            whose date falls before or after specified
            date.</para>
        </sect3> 
        <sect3 id="boolean">
            <title>Boolean Queries</title>
            <para>Support for basic boolean operators querying: 
            AND and NOT required.  OR is optional <citation><ulink
            url="http://www.mail-archive.com/nutch-developers@lists.sourceforge.net/mail4.html#00000">[nutch-dev] archives</ulink>.  Search for 
            'Re: [Nutch-dev] AND/OR/NOT do not work?'.</citation>.</para>
        </sect3>
        <sect3 id="proximity">
            <title>Support for Proximity Querying</title>
            <para>Support for proximity specifiers: i.e. Two query terms
            occur within X terms of each other.</para>
        </sect3>

        <sect3 id="query_fields">
            <title>Narrow query to explicit Content-Type, Language, etc.</title>
            <para>It must be possible to narrow queries 
            by specifying document language or content-type or even size
            or URL content
            The fields  that can be used narrowing the query can only come
            of those listed in
            <xref linkend="hititem" />.
            .</para>
        </sect3>
       </sect2> 

       <sect2 id="results"> <title>Search Results</title>

       <sect3 id="results_xml">
        <title>Results as valid XHTML (Or XML)</title> 
       <para>Search results must be returned as HTML (or XHMTL) for humans
       and/or as valid XHTML (Or XML with Schema) so results are easily
       digestible by programs.
       </para>
       </sect3>

        <sect3 id="searchresultsummary"><title>Summary of Results</title>
            <para>Every search should summarize the search results at the
            top of the page.  At a minimum there should be  total
            number of results found.
            </para>
        </sect3>

        <sect3 id="pagable">
            <title>Search Results should be Pageable/Navigable</title>
                <para>Number of results to return should be settable as
                part of the query.
                It must also be possible to specify a range of search
                results to return so its possible to implement paging
                through results.</para>
        </sect3>
        <sect3 id="sortable">
            <title>Sortable Results</title>
            <para>Sort results by date -- ascending and descending.</para>
        </sect3>

        <sect3 id="hititem">
            <title>Search Result Item</title>
            <para>A search result hit should comprise at least the following
            fields:
                <orderedlist>
                    <listitem><para>Document ID. Must be unique within
                    the web archive.</para></listitem>
                    <listitem><para>Document title (links to found
                    document).</para></listitem>
                    <listitem><para>URL that points at document original
                    -- or to a service that can rendor or knows how to 
                    fetch the indexed
                    document.</para></listitem>
                    <listitem><para>Document size</para></listitem>
                    <listitem><para>Document type</para></listitem>
                    <listitem><para>Crawl date</para></listitem>
                    <listitem><para>Document summary highlighting
                    found terms.</para></listitem>
                    <listitem><para>Link to other versions of this document
                    that differ by crawl-date only or by crawl-date and
                    content. This link should not show if there are no other
                    versions of this document.</para></listitem>
                </orderedlist>
            </para>
            <para>Other fields to include in results would be
            <literal>collection</literal> -- the repository 'collection'
            the found document belongs to -- and docment content-type
            and language.
            </para>
        </sect3>

        <sect3 id="expected"><title>Further Expected Search Result
        Behavior</title>
        <sect4 id="website"><title>URL contains Query Term</title>
            <para>If query term appears in an URL, the website should show
            first in the list of results.  For example, typing in
            <literal>archive</literal>
            should return the home page for the internet archive as first
            search result item because its URL is 
            <literal>http://www.archive.org/</literal>: i.e. it contains
            the query term.</para>
        </sect4>
        <sect4 id="multi_same_url"><title>Multiple hits of same URL</title>
            <para>Only show most recently-crawled in search results.</para>
            <para>If more than one hit, indicate in search result hit
            item that there are 'more' or 'other versions'. 
            This indicator of 'more' or 'other versions' should not
            show if there are not 'more', if only one URL found.
            The 'more' or 'other versions' returns a page
            listing all instances of the hit URL that live in the 
            indexed collection.
            </para>

            <para>Optionally, a configuration would enable all same-URL
            hits clustered together in the search
            results with the most recent appearing first.  A rendoring
            program could draw all but the first result right-shifted.
            </para>

            <note><title>Nutch deduping</title>
            <para>Nutch has the <literal>segmentmerge</literal> tool that 
            removes all pages with same URL. If same content -- same 
            md5 -- then pages need to be marked for deletion from the webdb.
            Need to do this as a batch process.
            </para>
            </note>

            <sect5><title>Same URL, same content, different crawltime</title>
                <para>Only the most recently crawled shows in search 
                results with 'more' or 'other versions' link to 
                a page that lists all other instances that match.</para>
            </sect5>

            <sect5><title>Same URL, different content,
                    different crawltime</title>
                <para>Both documents should show in search results list if
                they satisfy the query terms.
                (To be confirmed: Are such pages searchable by nutch?
                Currently nutch
                deduping is eliminating content that has same URL?).</para>
            </sect5>
        </sect4>
        <sect4 id="same_content_diff_url"><title>Same content, different URL</title>
            <para>Only the 
            <emphasis>canonical</emphasis> version should be shown
            (TODO: Define canonical).</para>
        </sect4>
        </sect3>

<sect3 id="configurableresults"><title>Configurable Search Results</title>
       <sect4 id="configurablebaseurl"><title>Configurable Hit Result
       Base URL</title>
       <para>The search result base URL -- the 'wayback pointer' or
       pointer into NWA storage -- must be configurable.
       </para>
       <para>Search result hits will point at a wayback-like server. This
       server will act as the gateway to the IA or NWA Toolkit
       content repository.
       It pulls the found content when a user clicks on a search result.  
       The location of this server will vary independent
       of index writing so it must be possible to change the query engine to
       use a different wayback-like server just by altering a simple
       configuration.  The result of altering this configuration is that the
       query results will subsequently point at the new wayback location.
       Changing this configuration should not necessitate rebuilding or undoing and
       rebundling war files.  Changing a configuration file and restarting
       the application would be sufficent.
       </para>
       </sect4>
       </sect3>


    </sect2>
    <sect2 id="indexing">
        <title>Indexing</title>
            <sect3 id="supported_doctype">
            <title>Required Document Types</title>
            <para>txt, html, xml, and NWA Document Format type</para>
            <sect4 id="optional"><title>Optional Document Types</title>
            <para>Optionally index pdf and msword.</para>
            </sect4>
            </sect3>
            <sect3 id="incremental"><title>Incremental Indexing</title>
                <para>The ability to add documents on an incremental basis 
                to an extant index without have to rebuild the total index
                from scratch is a requirement.
                </para>
            </sect3>
            <sect3 id="fieldstoindex"><title>Fields to Index</title>
                <para>ARC content fields will be determined by nutch plugins:
                E.g. The nutch text/html plugin will pull out document title
                and all of the document text.
                </para>
                <para>The lucene NWA Toolset search engine
                sums together all of the NWA Document Format instances'
                xml into a text field for indexing and to run queries against.
                The NWA collection bridge will do the equivalent or better.
                </para>
            </sect3>
    </sect2>
    <sect2 id="performance">
        <title>Performance</title>
        <sect3 id="querying">
            <para>Queries against an index of 100million documents should
            take no longer than 1 second returning a page of results
            <emphasis>on average</emphasis>
            (TODO: Specify hardware).
            </para>
        </sect3>
        <sect3 id="indexingperf"><title>Indexing Performance</title>
            <para>Indexing txt and html only, 
            processing must run at no less than one gig of
            compressed ARCs every X seconds.</para>
        </sect3>
        <sect3 id="indexsize"><title>Index Size</title>
        <para>Its required that it index 100million txt/html documents.</para>
        </sect3>
        <sect3 id="indexsize_relative"><title>Index Size relative to Content Size</title>
            <para>Index size cannot be greater than 25% that of the
            original content (Assuming an index of html/txt/xml only).</para>
        </sect3>
    </sect2>

    <sect2 id="nutch_customizations"><title>Required Nutch
        Customizations</title>
        <para>Apart from the above mentioned 'bridges' <citation>
        <xref linkend="doc_src" /></citation> between the
        document collections and nutch, the following customizations will be
        required.
        </para>

    <sect3><title>No Caching of Raw Content</title>
        <para>Caching of raw content by nutch duplicates what is already in
        the archive.  Its required that duplicates not be maintained in 
        nutch.  Must be possible to run a cron on a period to clean segments
        of content or else nutch needs to do this when indexing is done or
        else, never write this duplicate content in the first place.
        </para>
    </sect3>
    <sect3 id="inlinking"><title>Inbound links</title>
    <para>Scripts that can count inbound links and exploit link text
    -- facility already present in nutch but by-passed by our
    non-nutch-fetcher usage -- 
    boosting docs with a high-count of inbound links.
    </para>
    </sect3>
    </sect2>
  </sect1>
      <appendix id="nwatoolset"><title>
      <ulink url="http://nwatoolset.sourceforge.net/">NWA Toolset</ulink></title>
      <para>A toolkit created by our partners at the Norwegian Library of
      Norway.  See <ulink
      url="http://nwatoolset.sourceforge.net/">NWA Toolkit</ulink>.
      </para>
      <para id="nwadf">
      The <emphasis>NWA Document Format</emphasis>, in summary, is an XML
      document that holds the
      text extracted from downloaded web content conflated with metadata about
      the downloaded content.
      The NWA Toolset can use FAST or lucene as its search engine 
      plugin.  The lucene implementation is rudimentary.
      There is a webapp within which you designate
      where on the filesystem the NWA Document Format instances reside.
      The webapp then proceeds to pick the xml files from disk 
       and feeds them to lucene.
      </para>
      <para>Notes from an NWA Toolkit review can be found here,
      <ulink url="http://crawler.archive.org/cgi-bin/wiki.pl?NwaToolSet">Notes Reviewing NWA Toolset 1.0.0RC10, October 22nd, 2004</ulink>.
      </para>
      </appendix>

      <appendix id="crawlprojects"><title>CrawlProjects</title>
      <para>A demo application that uses
      <ulink url="http://nutch.org">nutch</ulink>
      to search an ARC repository of ~2 million documents exists at
      <ulink url="">crawlprojects.archive.org</ulink>.</para>
      </appendix>

        <appendix id="toolset">
        <title><ulink url="http://member.netpreserve.org/wws/d_read/framework/IIPC_framework_ver1-1.doc">An Architectural Framework for
            the IIPC Toolset: 5. Search</ulink></title>
            <para>(Requires login)
            This document outlines the search component of the IIPC Toolset.
            It also talks of the as yet-to-be-defined
            <xref linkend="iipc_aip" />
            document type as well as to-be-developed 
            APIs to abstract indexing and searching of search engine
            particulars.</para>
        </appendix>

      <appendix id="iipc_aip"><title>IIPC Archive Information Package</title>
      <para>This document format is yet-to-be defined.
      Such documents will likely be XML made of the document
  main attributes, the stripped text, and metadata (i.e. Similar to
  <xref linkend="nwadf" />).
  These document-types are described in <xref linkend="toolset" />.

      </para>
      </appendix>



</article>
